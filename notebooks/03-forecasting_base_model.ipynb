{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b35c11e",
   "metadata": {},
   "source": [
    "# Base Model for Demand Forecasting\n",
    "\n",
    "The main objective is to optimize inventory and purchasing management, with a target of **reducing overstocking by 20%** within 6 months.\n",
    "\n",
    "- Target Variable for Inventory Optimization: **Stock_Quantity**\n",
    "- Target Variable for Demand Forecasting: **Sales_Volume**\n",
    "\n",
    "### Metrics for models avaliation\n",
    "- RMSE - Root Mean Squared Error\n",
    "- MAE - Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecca068",
   "metadata": {},
   "source": [
    "# DATA ACQUISITION\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standart Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Specialized Libraries\n",
    "import mlflow\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Class and functions \n",
    "from smart_supply_chain_ai.utils.functions import DateFeatureExtractor, Differentiator, TextTokenizer\n",
    "\n",
    "# Notebook mlflow Loggings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b4383",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_path = os.path.join('../data', 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path + '/grocery.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3448dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e1d9c",
   "metadata": {},
   "source": [
    "# Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbe80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features\n",
    "df['day_of_week'] = df['Date_Received'].dt.dayofweek.astype('category')\n",
    "df['month'] = df['Date_Received'].dt.month.astype('category')\n",
    "df['year'] = df['Date_Received'].dt.year.astype('category')\n",
    "df['day_of_year'] = df['Date_Received'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying differentiation in non stationary variables\n",
    "df['Delivery_Lag_diff'] = df['Delivery_Lag'].diff().fillna(0)\n",
    "df['Days_For_Expiration_diff'] = df['Days_For_Expiration'].diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ascending date\n",
    "df = df.sort_values(by='Date_Received').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ffcae",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Columns\n",
    "y = df[['Sales_Volume', 'Stock_Quantity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d649ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For modeling, we removed highly correlated columns and unique identifiers that did not add predictive value.\n",
    "drop_columns = ['Product_ID', 'Supplier_ID', 'Last_Order_Date', 'Expiration_Date',\n",
    "       'Warehouse_Location', 'Stock_Value', 'Days_For_Expiration', \n",
    "       'Purchase_Order', 'Delivery_Lag'] + y.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4608b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Columns\n",
    "X= df.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32633042",
   "metadata": {},
   "source": [
    "## Encode Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff6d1a",
   "metadata": {},
   "source": [
    "### One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Non numeric Variables\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37988bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns for One Hot\n",
    "columns_ = ['Category', 'Status', 'Expiration_Status']\n",
    "\n",
    "# Fit in X_train\n",
    "encoder.fit(X_train[columns_])\n",
    "\n",
    "# Transform X \n",
    "X_train_encoded = encoder.transform(X_train[columns_])\n",
    "X_test_encoded = encoder.transform(X_test[columns_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoded dataframes\n",
    "encoded_columns_name = encoder.get_feature_names_out(columns_)\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoded_columns_name, index=X_train.index)\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoded_columns_name, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ce480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union datasets\n",
    "X_train = pd.concat([X_train.drop(columns=columns_), X_train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=columns_), X_test_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8454c0",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the columns in list of list\n",
    "X_train_to_hash = [[prod, supp] for prod, supp in zip(X_train['Product_Name'], X_train['Supplier_Name'])]\n",
    "X_test_to_hash = [[prod, supp] for prod, supp in zip(X_test['Product_Name'], X_test['Supplier_Name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Hashing\n",
    "n_features=100\n",
    "hasher = FeatureHasher(n_features=n_features, input_type=\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1750b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appling\n",
    "X_train_hashed = hasher.transform(X_train_to_hash)\n",
    "X_test_hashed = hasher.transform(X_test_to_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns Names for data\n",
    "hashed_column_names = [f'hashed_feature_{i}' for i in range(n_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with dense matrix\n",
    "X_train_hashed_df = pd.DataFrame(X_train_hashed.toarray(), columns=hashed_column_names, index=X_train.index)\n",
    "X_test_hashed_df = pd.DataFrame(X_test_hashed.toarray(), columns=hashed_column_names, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dbc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Final DataFrame\n",
    "# Columns to remove\n",
    "columns_rm = X_train[['Product_Name', 'Supplier_Name']].columns.to_list()\n",
    "\n",
    "# Concatenate wit others DataFrames\n",
    "X_train_final = pd.concat([\n",
    "    X_train.drop(columns=columns_rm),\n",
    "    X_train_hashed_df\n",
    "], axis=1)\n",
    "\n",
    "X_test_final = pd.concat([\n",
    "    X_test.drop(columns=columns_rm),\n",
    "    X_test_hashed_df\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6596d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f5df9",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980664a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data\n",
    "df = pd.read_pickle(data_path + '/grocery.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6076000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train test\n",
    "X = df.drop(columns=[\"Sales_Volume\", \"Stock_Quantity\", 'Product_ID', 'Supplier_ID',\n",
    "                     'Stock_Value', 'Purchase_Order', 'Last_Order_Date', 'Expiration_Date', 'Warehouse_Location'])\n",
    "y = df[[\"Sales_Volume\", \"Stock_Quantity\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ccb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate Class\n",
    "extractor = DateFeatureExtractor(date_column='Date_Received')\n",
    "diff = Differentiator(columns=['Delivery_Lag', 'Days_For_Expiration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "X_train_processed = extractor.transform(X_train)\n",
    "X_train_processed = diff.transform(X_train_processed)\n",
    "\n",
    "X_test_processed = extractor.transform(X_test)\n",
    "X_test_processed = diff.transform(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "category_columns = X_train_processed.select_dtypes('category').columns.to_list()\n",
    "string_columns = X_train_processed.select_dtypes('object').columns.to_list()\n",
    "numeric_columns = (X_train_processed.select_dtypes(['int', 'float']).columns.to_list())\n",
    "# + (y_test.select_dtypes(['int', 'float']).columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2030a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers= [\n",
    "        # Encapsulate the TextTokenizer and FeatureHasher in a Pipeline\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('tokenizer', TextTokenizer()),\n",
    "            ('hasher', FeatureHasher(n_features=100, input_type='string'))\n",
    "        ]), string_columns),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), category_columns),\n",
    "        ('scale', StandardScaler(), numeric_columns)\n",
    "    ],\n",
    "    remainder='drop' # Remove columns that aren't in the list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc7d81",
   "metadata": {},
   "source": [
    "# Configure Pipelines and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ab4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configurations\n",
    "seed_std = 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritms for train\n",
    "# MultiOutputRegressor used because have more than 1 targets\n",
    "pipelines = {\n",
    "    'RandomForest': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(RandomForestRegressor(random_state=seed_std, oob_score=True)))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(LGBMRegressor(random_state=seed_std)))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(XGBRegressor(random_state=seed_std, silent=1)))\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fdda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with padronization for models\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'regressor__estimator__n_estimators': [200, 300, 400],\n",
    "        'regressor__estimator__max_depth': [None, 10, 20, 30],\n",
    "        'regressor__estimator__min_samples_split': [2, 5, 10],\n",
    "        'regressor__estimator__criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
    "        'regressor__estimator__max_features': ['sqrt', 'log2', None],\n",
    "\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'regressor__estimator__n_estimators': [200, 400, 600, 800],\n",
    "        'regressor__estimator__max_depth': [-1, 10, 20],\n",
    "        'regressor__estimator__learning_rate': [0.05, 0.1, 0.15],\n",
    "        'regressor__estimator__num_leaves': [31, 50, 70],\n",
    "        'regressor__estimator__colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "    'regressor__estimator__n_estimators': [200, 400, 600, 800],\n",
    "    'regressor__estimator__learning_rate': [0.05, 0.1, 0.15],\n",
    "    'regressor__estimator__max_depth': [3, 5, 10], # Geralmente valores menores que LGBM\n",
    "    'regressor__estimator__subsample': [0.7, 0.8, 1.0], # Subamostragem de linhas\n",
    "    'regressor__estimator__colsample_bytree': [0.7, 0.8, 1.0], # Subamostragem de colunas\n",
    "    'regressor__estimator__reg_alpha': [0, 0.1, 0.5] # Regularização L1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross validation for Time Series\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316df0e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Running MLflow\n",
    "\n",
    "To **start the MLflow** user interface, open your terminal and execute the following command: \n",
    "\n",
    "```\n",
    "cd notebooks\n",
    "pdm run mlflow ui\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModelPipeline(pipelines: dict, param_grids: dict, tscv, X, y, experiment_name: str):\n",
    "    \"\"\"\n",
    "    Trains and tunes multi-output models from a dictionary of pipelines,\n",
    "    logging results to MLflow using nested runs.\n",
    "    \n",
    "    Args:\n",
    "        pipelines (dict): Dictionary where keys are model names and values are scikit-learn pipelines.\n",
    "        param_grids (dict): Dictionary with the hyperparameter grid for searching.\n",
    "        tscv: TimeSeriesSplit object for cross-validation.\n",
    "        X: DataFrame with the training features.\n",
    "        y: DataFrame or array with the training targets (multi-output).\n",
    "\n",
    "    Example of use:\n",
    "        Assuming you already have X_train_processed, y_train, and tscv:\n",
    "            TrainModelPipeline(\n",
    "                pipelines=pipelines, \n",
    "                param_grids=param_grids, \n",
    "                tscv=tscv, \n",
    "                X=X_train_processed, \n",
    "                y=y_train, \n",
    "                experiment_name=\"Demand_Forecasting_Experiment\"\n",
    "            )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Starts the main run that will group all trainings\n",
    "    with mlflow.start_run(run_name=\"Hyperparameter_Optimization_Experiment\"):\n",
    "        print(\"Starting main MLflow run for hyperparameter optimization.\")\n",
    "\n",
    "        # Iterates over each model and its set of parameters\n",
    "        for model_name, current_pipeline in pipelines.items():\n",
    "            print(f'Starting process for model: {model_name}')\n",
    "\n",
    "            # Defines the name of the nested run\n",
    "            run_name = f\"GridSearch_{model_name}\"\n",
    "\n",
    "            # Checks if the run is already finished to avoid reprocessing\n",
    "            existing_runs = mlflow.search_runs(\n",
    "                experiment_names=[experiment_name],\n",
    "                filter_string=f\"tags.mlflow.runName = '{run_name}' and status = 'FINISHED'\"\n",
    "            )\n",
    "\n",
    "            if not existing_runs.empty:\n",
    "                print(f\"Skipping {model_name}... Run '{run_name}' already exists and is finished.\")\n",
    "                continue\n",
    "\n",
    "            # Gets the specific parameter dictionary for the current model\n",
    "            current_param_grid = param_grids.get(model_name, {})\n",
    "            if not current_param_grid:\n",
    "                print(f\"Warning: No param_grid found for {model_name}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Defines the scorer using RMSE\n",
    "            rmse_scorer = make_scorer(root_mean_squared_error, greater_is_better=False, multioutput='uniform_average')\n",
    "\n",
    "            # Starts the nested run for the current model\n",
    "            with mlflow.start_run(run_name=run_name, nested=True):\n",
    "                # Activates autologging for sklearn within the run context\n",
    "                mlflow.sklearn.autolog()\n",
    "                \n",
    "                # Initializes and fits GridSearchCV\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=current_pipeline,\n",
    "                    param_grid=current_param_grid,\n",
    "                    scoring=rmse_scorer,\n",
    "                    cv=tscv,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1,\n",
    "                    return_train_score=False\n",
    "                )\n",
    "                \n",
    "                print(f\"Fitting GridSearchCV for {model_name}...\")\n",
    "                grid_search.fit(X, y)\n",
    "                \n",
    "                # Autologging already logs most results\n",
    "                # Here you can log extra metrics or information\n",
    "                mlflow.log_metric(\"best_validation_rmse\", -grid_search.best_score_)\n",
    "                mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "                print(f\"Best RMSE for {model_name}: {-grid_search.best_score_}\")\n",
    "                print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a274647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Pipeline\n",
    "TrainModelPipeline(\n",
    "                pipelines=pipelines, \n",
    "                param_grids=param_grids, \n",
    "                tscv=tscv, \n",
    "                X=X_train_processed, \n",
    "                y=y_train, \n",
    "                experiment_name=\"Demand_Forecasting_Experiment\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc506e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4847cbe",
   "metadata": {},
   "source": [
    "## Train First Result without otimization\n",
    "\n",
    "Random Forest:  \n",
    "> MAE: ~21.8839 / RMSE: ~25.3878\n",
    "\n",
    "LightGBM:  \n",
    "> MAE: ~22.5347 /  RMSE: ~26.6251\n",
    "\n",
    "XGBoost: \n",
    "> MAE: ~23.4955 / RMSE: ~27.9475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Models\n",
    "# # MLflow configurations\n",
    "# experiment_name = 'Base_models:Demand_Forecasting'\n",
    "# mlflow.set_experiment(experiment_name)\n",
    "# # Experiment: 2_Hyperparameter_Tuning  \n",
    "# # setup_experiment('hyperparameter_tuning')\n",
    "# # Experiment: 3_Advanced_Models_DL\n",
    "# # setup_experiment('advanced_models')\n",
    "# # Experiment: 4_Final_Ensemble  \n",
    "# # setup_experiment('final_ensemble')\n",
    "\n",
    "\n",
    "# for model_name, current_pipeline in pipelines.items():\n",
    "#     run_name = f\"Training_{model_name}\"\n",
    "\n",
    "#     # Verify name and status\n",
    "#     existing_runs = mlflow.search_runs(\n",
    "#         experiment_names= [experiment_name],\n",
    "#         filter_string=f\"tags.mlflow.runName = '{run_name}' and status = 'FINISHED'\"\n",
    "#     )\n",
    "\n",
    "#     if not existing_runs.empty:\n",
    "#         print(f\"Skipping {model_name}... Run '{run_name}' already exists and is finished.\")\n",
    "#         continue\n",
    "\n",
    "#     with mlflow.start_run(run_name=run_name):\n",
    "#         print(f\"Training {model_name}...\")\n",
    "        \n",
    "#         # Train the pipeline\n",
    "#         current_pipeline.fit(X_train_processed, y_train)\n",
    "\n",
    "#         # Make predictions and evaluate\n",
    "#         preds = current_pipeline.predict(X_test_processed)\n",
    "#         rmse = root_mean_squared_error(y_test, preds)\n",
    "#         mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "#         # Log metrics and model with MLflow\n",
    "#         mlflow.log_metric(\"test_rmse\", rmse)\n",
    "#         mlflow.log_metric(\"test_mae\", mae)\n",
    "#         mlflow.sklearn.log_model(current_pipeline, f\"{model_name}_model\")\n",
    "\n",
    "#         print(f\"  Test RMSE for {model_name}: {rmse:.2f}\")\n",
    "\n",
    "# print(\"\\nModel comparison completed. See the results in the MLflow UI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.search_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6c296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381e15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f3bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c73e7004",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ee8e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "506038a2",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc43c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-supply-chain-ai-3.11 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
